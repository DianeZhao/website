---
title: "Pixelbots: Pixels with personality"
project_id: phd-pixelbots # ID of the project, used by publications to display in this project.
belongs_to_areas: [social-robots, entertainment] # List of area IDs, separated by commas.
date: 2010-09-01
end_date: 2014-01-31
description: >- # >- this means to ignore newlines until next field. This is the project description, displayed in the project's card"
  This project introduces a new interactive form of display termed as Pixelbots which can be used to project human movements onto these fleet of bots. Various human gestures and movements can be applied to this novel display which provides an interaction between human and display systems which have not been considered in conventional technologies. 
image: /assets/images/projects/Pixelbots.png
# links: # If you have a website for the project, repos, etc. put it here.
#     - name: "Github: multi-robot collision avoidance"
#       url: "https://github.com/tud-amr/mrca_vc"
#     - name: "Github: MPCC implementation"
#       url: "https://github.com/tud-amr/amr-lmpcc"
fundings: This project was funded by Disney Research Zurich
people: # If you put your name in the same way you have it in your _people entry, your preferred link will be added. extra_info is optional.
    
      # extra_info: PhD candidates
    - name: Roland Siegwart
      extra_info: Autonomous Systems Lab, ETH Zurich
    - name: Paul Beardsley
      extra_info: Disney Research Zurich
    - name: Prof. Javier Alonso-Mora
      extra_info: Autonomous Systems Lab, ETH Zurich
      # extra_info: Intelligent Vehicles (IV) Group TU Delft
---
<!-- Here you put the main body of the page, in markdown. You can also mix in html, or change this .md to .html -->
<!-- The fields of People, Funding, Links and Publications will be generated automatically -->

## About the Project

In todayâ€™s digital media landscape, we are constantly surrounded by displays, from the LCDs found on the phones in our pockets to the ubiquitous screens that greet us whenever we enter a store, airport or educational institution. This project aims to take these displays and enable human gestures, movements and abilties to be applied to them. This helps bring displays into a kinetic form and break the stereotypical rectangular or oval experience for digital displays. Pixels become mobile entities and their positioning and motion are used to produce a novel user experience.  

This has been done through the use of robotic displays which basically comprises of a robot swarm where each robot can be remotely controlled and is equivalent to one mobile pixel with RGB LED for controllable color. This display can then be used to reproduce images or animations through a goal generation algorithm that computes a goal position for each mobile pixel. A control loop is then used to firstly allocate positions to each robot, compute a velocity and enable collision avoidance so that the mobile pixels can quickly reach their desired location.   

The next step in the project is to then enable gesture based human control over this robot swarm. This has been done by providing two modes of interaction: free-form and shape contrained. In free-form interaction, users can select individual robots and change their positions or move them along a trajectory. In shape-constrained interaction, user can only control a subset of the degrees of freedom so that the configuration is maintained. This work can be extended to several essential applications in addition to entertainment such as surveillance or search-and-rescue operations.     

## Project Demonstrations

<div class="video-wrapper ratio ratio-16x9"> 
  <iframe width="560" height="315" src="https://www.youtube.com/embed/4-3rkrqvO14?si=FDkcJrxZe2Ra9A8O&mute=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>
<div class="video-wrapper ratio ratio-16x9">  
  <iframe width="560" height="315" src="https://www.youtube.com/embed/bM5-nyeNjms?si=QvZoeOgNu0-JQ5an&mute=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div>

## Funding & Partners

This project has received funding from the Netherlands Organisation for Scientific Research (NWO) Applied Sciences with project Veni 15916. Views and opinions expressed are, however, those of the author(s) only and do not necessarily reflect those of the NWO. Neither the NWO nor the granting authority can be held responsible for them.
